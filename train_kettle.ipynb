{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "USE_SIMULATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_v2_behavior()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 200000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 10  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 16  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 1000  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for the real kettle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe_kettle():\n",
    "    # TODO remove the offset!!\n",
    "    resp = requests.get(\"http://192.168.178.87/\")\n",
    "    a = json.loads(resp.text)\n",
    "    temperatures = [x[1]-50.0 for x in json.loads(resp.text)[\"temp\"]]\n",
    "    on_off = [float(x[1]) for x in json.loads(resp.text)[\"state\"]]\n",
    "    return temperatures+on_off\n",
    "\n",
    "def turn_kettle_on():    \n",
    "    resp = requests.get(\"http://192.168.178.87/r_on\")\n",
    "    \n",
    "def turn_kettle_off():    \n",
    "    resp = requests.get(\"http://192.168.178.87/r_off\")\n",
    "\n",
    "    \n",
    "# observation = observe_kettle()\n",
    "# print(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated kettle environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class KettleEnv(py_environment.PyEnvironment):\n",
    "  def __init__(self):\n",
    "    self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
    "    self._observation_spec = array_spec.BoundedArraySpec(shape=(20,), dtype=np.float32, minimum=0, name='observation')\n",
    "    self._state = 0\n",
    "    self._episode_ended = False\n",
    "\n",
    "  def action_spec(self):\n",
    "    return self._action_spec\n",
    "\n",
    "  def observation_spec(self):\n",
    "    return self._observation_spec\n",
    "\n",
    "  def _reset(self):\n",
    "    # TODO print something for me to clean out the kettle...\n",
    "    self._state = observe_kettle()\n",
    "    self._episode_ended = False\n",
    "    return ts.restart(np.array(self._state, dtype=np.float32))\n",
    "\n",
    "  def _step(self, action):\n",
    "    if self._episode_ended:\n",
    "      # The last action ended the episode. Ignore the current action and start a new episode.\n",
    "      return self.reset()\n",
    "\n",
    "    # Make sure episodes don't go on forever.\n",
    "    if action == 1:\n",
    "        turn_kettle_on()\n",
    "    elif action == 0:\n",
    "        turn_kettle_off()\n",
    "    else:\n",
    "      raise ValueError('`action` should be 0 or 1.')\n",
    "\n",
    "    # TODO do I have to sleep here or somewhere else...\n",
    "    time.sleep(1.0)\n",
    "    \n",
    "    self._state = observe_kettle()\n",
    "\n",
    "    current_temp = self._state[9]\n",
    "    print(\"Current temp\", current_temp)\n",
    "    given_reward = 0.0\n",
    "    if current_temp < 0:\n",
    "        given_reward = 1.0-0.0*abs(current_temp)\n",
    "    else:\n",
    "        given_reward = 1.0-0.03*abs(current_temp)\n",
    "    \n",
    "    return ts.transition(np.array(self._state, dtype=np.float32), reward=given_reward, discount=1.0)\n",
    "\n",
    "\n",
    "\n",
    "class SimulatedKettleEnv(py_environment.PyEnvironment):\n",
    "  def __init__(self):    \n",
    "    self.action_history_seconds = 100\n",
    "    self.set_temperature = 50.0\n",
    "        \n",
    "    # Actions the neural network can take. In this caes it's either 0 or 1. \n",
    "    self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
    "    \n",
    "    # The observations the neural network makes. In this case it consists of: \n",
    "    #   * a certain length of history in temperatures in seconds\n",
    "    #   * a certain length of history in temperatures in action-performed (0 or 1)\n",
    "    self._observation_spec = array_spec.BoundedArraySpec(shape=(2*self.action_history_seconds,), dtype=np.float32, minimum=0, name='observation')\n",
    "\n",
    "    # Some other things. \n",
    "    self._state = 0\n",
    "    self._episode_ended = False\n",
    "\n",
    "  def action_spec(self):\n",
    "    return self._action_spec\n",
    "\n",
    "  def observation_spec(self):\n",
    "    return self._observation_spec\n",
    "\n",
    "  def _update_state(self):\n",
    "            # The current state consists of two things: \n",
    "#     self._state = self.water_history[-self.action_history_seconds:] + self.action_history[-self.action_history_seconds:]\n",
    "    self._state = [0.01*(x-self.set_temperature) for x in self.water_history[-self.action_history_seconds:]] + self.action_history[-self.action_history_seconds:]\n",
    "    \n",
    "  def _reset(self):\n",
    "    print(\"Resetting the kettle\")\n",
    "    # Reset the model of the rod and the water\n",
    "    self.temperature_rod = 20.0\n",
    "    self.temperature_water = 20.0\n",
    "    \n",
    "    # Reset the parameters which determine the state\n",
    "    self.water_history = [self.temperature_water]*self.action_history_seconds\n",
    "    self.action_history = [0.0]*self.action_history_seconds # pretend we did not do anything for a while\n",
    "    \n",
    "\n",
    "    \n",
    "    self._episode_ended = False\n",
    "    return ts.restart(np.array(self._state, dtype=np.float32))\n",
    "\n",
    "  def _step(self, action):\n",
    "    if self._episode_ended:\n",
    "      # The last action ended the episode. Ignore the current action and start a new episode.\n",
    "      return self.reset()\n",
    "\n",
    "    # TODO implement a better mathematical model, such as this one: https://www.ijesm.co.in/uploads/68/5720_pdf.pdf\n",
    "    if action == 1:\n",
    "        self.temperature_rod += 0.9\n",
    "    elif action == 0:\n",
    "        # don't do anything, rod will cool down anyways\n",
    "        pass\n",
    "    else:\n",
    "      raise ValueError('`action` should be 0 or 1.')\n",
    "\n",
    "    \n",
    "    # Model the heat dissipation to the environment\n",
    "    diff_temp = self.temperature_rod-self.temperature_water\n",
    "    \n",
    "    # I guess the rod holds quite a lot of energy and we have a small water basin (a.k.a. my setup at home)\n",
    "    # would be great to infer these parameters and make a better model\n",
    "    self.temperature_water += diff_temp*0.1\n",
    "    self.temperature_rod -= diff_temp*0.05 \n",
    "    \n",
    "    diff_water_air = self.temperature_water - 20.0\n",
    "    self.temperature_water -= diff_water_air*0.01 # very slowly dissipate the heat\n",
    "    \n",
    "    # Add the temperature of the water and the performed action\n",
    "    self.water_history.append(self.temperature_water)\n",
    "    self.action_history.append(float(action))\n",
    "    \n",
    "    self._update_state()\n",
    "    \n",
    "    # TODO Very ugly way to get the temperature to 30 degrees...\n",
    "#     print(\"Current temp\", self.temperature_water, ' action was ', action)\n",
    "    current_temp = self.temperature_water - self.set_temperature\n",
    "\n",
    "\n",
    "    given_reward = 0.0\n",
    "    if current_temp < 0:\n",
    "        given_reward = 1.0-0.01*abs(current_temp)\n",
    "    else:\n",
    "        given_reward = 1.0-0.03*abs(current_temp)\n",
    "    \n",
    "    if self.temperature_water > 90.0:\n",
    "        # This is too hot, better cool down a bit\n",
    "        self._episode_ended = True\n",
    "        return ts.termination(np.array(self._state, dtype=np.float32), given_reward)\n",
    "    else:\n",
    "        return ts.transition(np.array(self._state, dtype=np.float32), reward=given_reward, discount=1.0)\n",
    "    \n",
    "if USE_SIMULATION:\n",
    "    env = SimulatedKettleEnv()\n",
    "else:\n",
    "    env = KettleEnv()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the kettle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([-0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3,\n",
       "       -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3,\n",
       "       -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3,\n",
       "       -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3,\n",
       "       -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3,\n",
       "       -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3,\n",
       "       -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3,\n",
       "       -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3,\n",
       "       -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3, -0.3,\n",
       "       -0.3,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "        0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "        0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "        0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "        0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "        0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "        0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "        0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "        0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "        0. ,  0. ], dtype=float32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedArraySpec(shape=(200,), dtype=dtype('float32'), name='observation', minimum=0.0, maximum=3.4028234663852886e+38)\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Spec:\n",
      "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n"
     ]
    }
   ],
   "source": [
    "print('Reward Spec:')\n",
    "print(env.time_step_spec().reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Spec:\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=1)\n"
     ]
    }
   ],
   "source": [
    "print('Action Spec:')\n",
    "print(env.action_spec())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SIMULATION:\n",
    "    train_py_env = SimulatedKettleEnv()\n",
    "    eval_py_env = SimulatedKettleEnv()\n",
    "else:\n",
    "    train_py_env = KettleEnv()\n",
    "    eval_py_env = KettleEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_params = (30,10,5)\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.networks.q_network.QNetwork at 0x7f36b409f9b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    epsilon_greedy = 0.1,\n",
    "    td_errors_loss_fn=common.element_wise_huber_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SIMULATION:\n",
    "    example_environment = tf_py_environment.TFPyEnvironment(SimulatedKettleEnv())\n",
    "else:\n",
    "    example_environment = tf_py_environment.TFPyEnvironment(KettleEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the kettle\n"
     ]
    }
   ],
   "source": [
    "time_step = example_environment.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "def compute_avg_return(environment, policy, num_steps = 100):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "    \n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())\n",
    "collect_data(train_env, random_policy, replay_buffer, steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (Trajectory(step_type=(16, 2), observation=(16, 2, 200), action=(16, 2), policy_info=(), next_step_type=(16, 2), reward=(16, 2), discount=(16, 2)), BufferInfo(ids=(16, 2), probabilities=(16,))), types: (Trajectory(step_type=tf.int32, observation=tf.float32, action=tf.int32, policy_info=(), next_step_type=tf.int32, reward=tf.float32, discount=tf.float32), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset the train step\n",
      "Evaluate the agent's policy once before training.\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 1000: loss = 4251.89599609375, learning from 11000\n",
      "Temperature is 50.90662346643358\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 2000: loss = 47977.5078125, learning from 12000\n",
      "Temperature is 27.18214604754056\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 3000: loss = 311714.53125, learning from 13000\n",
      "Temperature is 86.15515233538837\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 4000: loss = 843254.875, learning from 14000\n",
      "Temperature is 70.07878205106307\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 5000: loss = 1411633.25, learning from 15000\n",
      "Temperature is 56.68756244753082\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 6000: loss = 2379895.25, learning from 16000\n",
      "Temperature is 38.7227244109269\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 7000: loss = 7100611.5, learning from 17000\n",
      "Temperature is 89.56381833026502\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 8000: loss = 7441929.5, learning from 18000\n",
      "Temperature is 77.69062975975626\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 9000: loss = 12524054.0, learning from 19000\n",
      "Temperature is 67.73390531402437\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 10000: loss = 18924468.0, learning from 20000\n",
      "Temperature is 51.918898365537984\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 11000: loss = 19615826.0, learning from 21000\n",
      "Temperature is 26.658724528774016\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 12000: loss = 12759538.0, learning from 22000\n",
      "Temperature is 83.31729763718653\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 13000: loss = 15213092.0, learning from 23000\n",
      "Temperature is 72.5169442287965\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 14000: loss = 12216131.0, learning from 24000\n",
      "Temperature is 56.837951789137335\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 15000: loss = 12552068.0, learning from 25000\n",
      "Temperature is 34.578598437114586\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 16000: loss = 16180054.0, learning from 26000\n",
      "Temperature is 30.268289533260536\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 17000: loss = 16663208.0, learning from 27000\n",
      "Temperature is 81.74585166518288\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 18000: loss = 13944336.0, learning from 28000\n",
      "Temperature is 64.96302744344688\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 19000: loss = 17744920.0, learning from 29000\n",
      "Temperature is 50.422143172329676\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 20000: loss = 26120708.0, learning from 30000\n",
      "Temperature is 35.64659191997657\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 21000: loss = 137280288.0, learning from 31000\n",
      "Temperature is 20.0891\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 22000: loss = 33184696.0, learning from 32000\n",
      "Temperature is 78.27895504868856\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 23000: loss = 39887752.0, learning from 33000\n",
      "Temperature is 71.63104485809856\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 24000: loss = 356307200.0, learning from 34000\n",
      "Temperature is 52.44674649406459\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 25000: loss = 48464592.0, learning from 35000\n",
      "Temperature is 40.22791863906954\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 26000: loss = 42147672.0, learning from 36000\n",
      "Temperature is 25.304208995388034\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 27000: loss = 48875696.0, learning from 37000\n",
      "Temperature is 86.2015251023367\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 28000: loss = 46660848.0, learning from 38000\n",
      "Temperature is 70.82371799681471\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 29000: loss = 41631600.0, learning from 39000\n",
      "Temperature is 52.46196190816898\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 30000: loss = 58801632.0, learning from 40000\n",
      "Temperature is 28.903405065316957\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 31000: loss = 37494800.0, learning from 41000\n",
      "Temperature is 82.87721818564063\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 32000: loss = 58095888.0, learning from 42000\n",
      "Temperature is 75.89651534974979\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 33000: loss = 55220692.0, learning from 43000\n",
      "Temperature is 60.19096171116076\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 34000: loss = 45743984.0, learning from 44000\n",
      "Temperature is 41.79956359635941\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 35000: loss = 50758316.0, learning from 45000\n",
      "Temperature is 20.2531331\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 36000: loss = 34164912.0, learning from 46000\n",
      "Temperature is 77.3139461653156\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 37000: loss = 41888856.0, learning from 47000\n",
      "Temperature is 62.429826252607874\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 38000: loss = 40213936.0, learning from 48000\n",
      "Temperature is 51.32761727929404\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 39000: loss = 43619288.0, learning from 49000\n",
      "Temperature is 37.1673906294222\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 40000: loss = 38083872.0, learning from 50000\n",
      "Temperature is 86.24665596027685\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 41000: loss = 30623836.0, learning from 51000\n",
      "Temperature is 65.77193609598962\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 42000: loss = 23407676.0, learning from 52000\n",
      "Temperature is 51.35904633092977\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 43000: loss = 30207156.0, learning from 53000\n",
      "Temperature is 37.17818664314505\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 44000: loss = 31927524.0, learning from 54000\n",
      "Temperature is 84.90612129911675\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 45000: loss = 31188276.0, learning from 55000\n",
      "Temperature is 79.07094170695498\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 46000: loss = 26344448.0, learning from 56000\n",
      "Temperature is 69.59116484033247\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 47000: loss = 21740576.0, learning from 57000\n",
      "Temperature is 53.17407229038804\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 48000: loss = 22450260.0, learning from 58000\n",
      "Temperature is 40.229891161214624\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 49000: loss = 13733978.0, learning from 59000\n",
      "Temperature is 82.04833259553476\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 50000: loss = 20382964.0, learning from 60000\n",
      "Temperature is 63.08236206253902\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 51000: loss = 21114800.0, learning from 61000\n",
      "Temperature is 50.96287245350764\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 52000: loss = 12468229.0, learning from 62000\n",
      "Temperature is 33.00217925006889\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 53000: loss = 16255614.0, learning from 63000\n",
      "Temperature is 80.0570063264252\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 54000: loss = 12312030.0, learning from 64000\n",
      "Temperature is 62.29556578689369\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 55000: loss = 9640194.0, learning from 65000\n",
      "Temperature is 43.24016214122085\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 56000: loss = 12062123.0, learning from 66000\n",
      "Temperature is 24.527856456409857\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 57000: loss = 5315072.0, learning from 67000\n",
      "Temperature is 89.02692467724579\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 58000: loss = 6913142.0, learning from 68000\n",
      "Temperature is 76.7791878197923\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 59000: loss = 6294008.0, learning from 69000\n",
      "Temperature is 59.17143248597759\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 60000: loss = 3576512.0, learning from 70000\n",
      "Temperature is 45.69142217286371\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 61000: loss = 4445943.0, learning from 71000\n",
      "Temperature is 20.2531331\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 62000: loss = 6163762.5, learning from 72000\n",
      "Temperature is 77.15307714009795\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 63000: loss = 2924925.0, learning from 73000\n",
      "Temperature is 28.013264129296427\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 64000: loss = 3748651.5, learning from 74000\n",
      "Temperature is 38.86433194439078\n",
      "step = 65000: loss = 2039352.75, learning from 75000\n",
      "Temperature is 36.77131124982336\n",
      "step = 66000: loss = 1345113.5, learning from 76000\n",
      "Temperature is 29.250574163883172\n",
      "Resetting the kettle\n",
      "step = 67000: loss = 1144444.25, learning from 77000\n",
      "Temperature is 39.19986308309863\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 68000: loss = 1005687.375, learning from 78000\n",
      "Temperature is 61.59228941633138\n",
      "step = 69000: loss = 201892.375, learning from 79000\n",
      "Temperature is 29.826343947736817\n",
      "step = 70000: loss = 30446.77734375, learning from 80000\n",
      "Temperature is 38.536800605523545\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 71000: loss = 1708.5615234375, learning from 81000\n",
      "Temperature is 63.81594024619074\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 72000: loss = 596.8450927734375, learning from 82000\n",
      "Temperature is 57.94857519325496\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 73000: loss = 168.12850952148438, learning from 83000\n",
      "Temperature is 20.595993849001097\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 74000: loss = 790.905029296875, learning from 84000\n",
      "Temperature is 49.65642664587012\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 75000: loss = 390.18572998046875, learning from 85000\n",
      "Temperature is 77.39997674787634\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 76000: loss = 304.05419921875, learning from 86000\n",
      "Temperature is 42.639253455147355\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 77000: loss = 10961.748046875, learning from 87000\n",
      "Temperature is 34.66122303532412\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 78000: loss = 34897.875, learning from 88000\n",
      "Temperature is 26.141782444544603\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n",
      "step = 79000: loss = 175788.8125, learning from 89000\n",
      "Temperature is 69.85886382961928\n",
      "Resetting the kettle\n",
      "Resetting the kettle\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "print(\"Reset the train step\")\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "print(\"Evaluate the agent's policy once before training.\")\n",
    "# avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "# returns = [avg_return]\n",
    "\n",
    "for current_iteration in range(num_iterations):\n",
    "#       agent.collect_policy._epsilon = 1.0\n",
    "#     try:\n",
    "    #   print(\"Collect a few steps using collect_policy and save to the replay buffer.\")\n",
    "      for _ in range(collect_steps_per_iteration):\n",
    "        collect_step(train_env, agent.collect_policy, replay_buffer)\n",
    "\n",
    "    #   print(\"Sample a batch of data from the buffer and update the agent's network.\")\n",
    "      experience, unused_info = next(iterator)\n",
    "      train_loss = agent.train(experience).loss\n",
    "      step = agent.train_step_counter.numpy()\n",
    "    #   print(\"Step is \", step)\n",
    "\n",
    "\n",
    "      if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}, learning from {2}'.format(step, train_loss, replay_buffer.num_frames().numpy()))\n",
    "        print(\"Temperature is\", train_py_env.temperature_water)\n",
    "\n",
    "    #   if step % eval_interval == 0:\n",
    "    #     avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    #     print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    #     returns.append(avg_return)\n",
    "    #     except Exception as e:\n",
    "    #         print(\"Damn you kettle\")\n",
    "    #         print(e)\n",
    "    #         time.sleep(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blaat = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blaat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = KettleEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = replay_buffer.gather_all().observation.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(observed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(replay_buffer.gather_all().reward.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
